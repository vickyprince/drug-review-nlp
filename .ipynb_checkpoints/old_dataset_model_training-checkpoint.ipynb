{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ad509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and Exploring Data\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_data_path = 'drugLibTrain_raw.tsv'\n",
    "test_data_path = 'drugLibTest_raw.tsv'\n",
    "\n",
    "# Reading the data\n",
    "train_data = pd.read_csv(train_data_path, sep='\\t')\n",
    "test_data = pd.read_csv(test_data_path, sep='\\t')\n",
    "\n",
    "# Displaying the first few rows of the training data to understand its structure\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4332c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the Data\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "train_data.fillna('', inplace=True)\n",
    "test_data.fillna('', inplace=True)\n",
    "\n",
    "# Additional preprocessing steps\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    # Tokenize and remove stop words\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Rejoin tokens into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to each text column\n",
    "for column in ['benefitsReview', 'sideEffectsReview', 'commentsReview']:\n",
    "    train_data[column] = train_data[column].str.lower().apply(preprocess_text)\n",
    "    test_data[column] = test_data[column].str.lower().apply(preprocess_text)\n",
    "    \n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca36b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Data Analysis and Feature Extraction\n",
    "#Combined Approach: Useful when the context of the text across different columns is related and can be considered as a whole.\n",
    "    \n",
    "\n",
    "# 1. Import Necessary Libraries for Analysis\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming train_data and test_data are already loaded and preprocessed\n",
    "\n",
    "# 2. Concatenate Text Columns (If needed)\n",
    "# Combining 'benefitsReview', 'sideEffectsReview', 'commentsReview' into a single column\n",
    "train_data['combined_text'] = train_data[['benefitsReview', 'sideEffectsReview', 'commentsReview']].agg(' '.join, axis=1)\n",
    "test_data['combined_text'] = test_data[['benefitsReview', 'sideEffectsReview', 'commentsReview']].agg(' '.join, axis=1)\n",
    "\n",
    "# 3. Exploratory Analysis: Word Frequency\n",
    "all_text = ' '.join(train_data['combined_text'])\n",
    "word_counts = Counter(all_text.split())\n",
    "most_common_words = word_counts.most_common(30)\n",
    "print(\"Most Common Words:\", most_common_words)\n",
    "\n",
    "words, counts = zip(*most_common_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 30 Most Common Words')\n",
    "plt.show()\n",
    "\n",
    "# 4. Apply TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can tune these parameters\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_data['combined_text'])\n",
    "test_tfidf = tfidf_vectorizer.transform(test_data['combined_text'])\n",
    "\n",
    "# Now, train_tfidf and test_tfidf are ready for use in machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Data Analysis and Feature Extraction\n",
    "#Separate Approach: Better when each text column has distinct contextual information that you want to capture individually.\n",
    "\n",
    "\n",
    "# 1. Import Necessary Libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Assuming train_data and test_data are already loaded and preprocessed\n",
    "\n",
    "# 2. Exploratory Analysis: Word Frequency\n",
    "# Combining text for exploratory analysis\n",
    "all_text = ' '.join(train_data[['benefitsReview', 'sideEffectsReview', 'commentsReview']].agg(' '.join, axis=1))\n",
    "word_counts = Counter(all_text.split())\n",
    "most_common_words = word_counts.most_common(30)\n",
    "print(\"Most Common Words:\", most_common_words)\n",
    "\n",
    "words, counts = zip(*most_common_words)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(words, counts)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 30 Most Common Words')\n",
    "plt.show()\n",
    "\n",
    "# 3. One-Hot Encoding for Categorical Columns\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_features_train = encoder.fit_transform(train_data[['effectiveness', 'sideEffects', 'condition']])\n",
    "encoded_features_test = encoder.transform(test_data[['effectiveness', 'sideEffects', 'condition']])\n",
    "\n",
    "# 4. Separate TF-IDF Vectorization for Text Columns\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=300)  # Adjusted for individual columns\n",
    "benefits_tfidf_train = tfidf_vectorizer.fit_transform(train_data['benefitsReview'])\n",
    "benefits_tfidf_test = tfidf_vectorizer.transform(test_data['benefitsReview'])\n",
    "\n",
    "sideEffects_tfidf_train = tfidf_vectorizer.fit_transform(train_data['sideEffectsReview'])\n",
    "sideEffects_tfidf_test = tfidf_vectorizer.transform(test_data['sideEffectsReview'])\n",
    "\n",
    "comments_tfidf_train = tfidf_vectorizer.fit_transform(train_data['commentsReview'])\n",
    "comments_tfidf_test = tfidf_vectorizer.transform(test_data['commentsReview'])\n",
    "\n",
    "# 5. Combining All Features\n",
    "train_features = hstack([benefits_tfidf_train, sideEffects_tfidf_train, comments_tfidf_train, encoded_features_train])\n",
    "test_features = hstack([benefits_tfidf_test, sideEffects_tfidf_test, comments_tfidf_test, encoded_features_test])\n",
    "\n",
    "# Now, train_features and test_features are ready for use in machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bd5ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(sentences):\n",
    "    # Tokenize and encode sentences for BERT\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Move to the same device as the model\n",
    "    encoded_input = encoded_input.to(model_bert.device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model_bert(**encoded_input)\n",
    "    \n",
    "    # Mean pool the token embeddings to get sentence embeddings\n",
    "    return output['last_hidden_state'].mean(dim=1)\n",
    "\n",
    "# Example usage\n",
    "bert_embeddings = get_bert_embeddings(train_data['combined_text'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414b2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
